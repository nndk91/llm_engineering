{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "402d5686-4e76-4110-b65a-b3906c35c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The issue you're encountering is indeed due to the lack of headers in your request. Here's an updated version of your code with headers included:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib.parse import urljoin, urlparse\n",
      "from time import sleep\n",
      "\n",
      "class Website:\n",
      "    def __init__(self, base_url):\n",
      "        self.base_url = base_url\n",
      "        self.headers = {\n",
      "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
      "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
      "            'Accept-Language': 'en-US,en;q=0.5',\n",
      "            'Connection': 'keep-alive',\n",
      "        }\n",
      "\n",
      "# Define the base URL and list of TaxIDs\n",
      "base_url = \"https://doanhnghiep.biz/\"\n",
      "tax_ids = [\"3502447502\", \"030352220\"]\n",
      "\n",
      "# Create an empty dictionary to store the scraped data\n",
      "scraped_data = {}\n",
      "\n",
      "# Iterate over each TaxID and update the URL\n",
      "for tax_id in tax_ids:\n",
      "    updated_url = base_url + str(tax_id)\n",
      "    \n",
      "    # Send a GET request to the updated URL with headers\n",
      "    response = requests.get(updated_url, headers=self.headers, timeout=5)\n",
      "    \n",
      "    # If the GET request is successful, scrape the data\n",
      "    if response.status_code == 200:\n",
      "        # Get the content of the page\n",
      "        page_content = response.content\n",
      "        \n",
      "        # Create a BeautifulSoup object and specify the parser\n",
      "        soup = BeautifulSoup(page_content, 'html.parser')\n",
      "        \n",
      "        # Find all elements on the page that contain text\n",
      "        for element in soup.find_all(text=True):\n",
      "            # Remove leading/trailing whitespace from the text\n",
      "            text = element.strip()\n",
      "            \n",
      "            # If the text is not empty, add it to the scraped data dictionary\n",
      "            if text:\n",
      "                scraped_data[text] = updated_url\n",
      "                \n",
      "    else:\n",
      "        print(f\"Failed to retrieve data for TaxID {tax_id}: status code {response.status_code}\")\n",
      "        \n",
      "    # Add a delay to avoid overwhelming the server\n",
      "    sleep(1)\n",
      "\n",
      "# Print the scraped data\n",
      "for key, value in scraped_data.items():\n",
      "    print(f\"{key}: {value}\")\n",
      "```\n",
      "\n",
      "However, if you're still encountering issues with 403 status codes after including headers, there are a few other things you can try:\n",
      "\n",
      "*   Ensure that your User-Agent string is not too similar to any existing User-Agent strings on the website. Websites may block requests from users with User-Agent strings that match those of bots or scripts.\n",
      "*   Use the `verify=False` parameter when creating the session, but be aware that this disables SSL certificate verification and makes your request more vulnerable to man-in-the-middle attacks.\n",
      "\n",
      "Here's how you can modify the code:\n",
      "\n",
      "```python\n",
      "session = requests.Session()\n",
      "session.verify = False\n",
      "\n",
      "# Send a GET request to the updated URL with headers\n",
      "response = session.get(updated_url, headers=self.headers, timeout=5)\n",
      "```\n",
      "\n",
      "*   Try accessing the website in your browser while using the User-Agent string from your script. If the website is blocking requests from users with that User-Agent string, you may need to change it.\n",
      "\n",
      "Here's an example of a User-Agent string that is less likely to be blocked:\n",
      "\n",
      "```python\n",
      "self.headers = {\n",
      "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.37',\n",
      "}\n",
      "```\n",
      "\n",
      "*   Use a more advanced User-Agent string generator library such as `fake-useragent` or `requests-faux-user-agent`.\n",
      "\n",
      "Here's an example of how you can use the `fake-useragent` library:\n",
      "\n",
      "```python\n",
      "from fake_useragent import UserAgent\n",
      "\n",
      "ua = UserAgent()\n",
      "self.headers = {\n",
      "    'User-Agent': ua.random,\n",
      "}\n",
      "```\n",
      "\n",
      "*   Inspect your requests using your browser's developer tools and check if there are any issues with your headers.\n",
      "\n",
      "Here's an example of how to use the developer tools:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "# Send a GET request to the updated URL with headers\n",
      "response = requests.get(updated_url, headers=self.headers, timeout=5)\n",
      "print(response.headers)\n",
      "```\n",
      "\n",
      "*   Use the developer tools on the website you're scraping and check if there are any issues with your User-Agent string.\n",
      "\n",
      "Here's an example of how to use the developer tools:\n",
      "\n",
      "```python\n",
      "# Open the website in your browser while using the User-Agent string from your script.\n",
      "# Go to chrome://settings/ and enable Developer mode.\n",
      "# Then, go to chrome://extensions/ and click \"Toggle developer mode\".\n",
      "# Click \"Load unpacked\" and select the folder that contains your script's HTML file.\n",
      "# Restart Chrome after enabling the extension.\n",
      "# Open a new tab in the browser while using the User-Agent string from your script.\n",
      "\n",
      "# Inspect the requests sent by your browser by going to chrome://neteer/ or chrome://network/.\n",
      "# Go to the website you're scraping and click on the \"Network\" tab.\n",
      "# Refresh the page and inspect the request with the wrong User-Agent string.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create your prompts\n",
    "import requests\n",
    "import ollama\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "#create message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "     \"\"\"\n",
    "   Fixing Failed to retrieve data for TaxID 3502447502: status code 403 in the below python code. This may be because url does not have headers\n",
    "   \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from time import sleep\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "\n",
    "# Define the base URL and list of TaxIDs\n",
    "base_url = \"https://doanhnghiep.biz/\"\n",
    "tax_ids = [\"3502447502\", \"030352220\"]\n",
    "\n",
    "# Create an empty dictionary to store the scraped data\n",
    "scraped_data = {}\n",
    "\n",
    "# Iterate over each TaxID and update the URL\n",
    "for tax_id in tax_ids:\n",
    "    updated_url = base_url + str(tax_id)\n",
    "    \n",
    "    # Send a GET request to the updated URL\n",
    "    response = requests.get(updated_url, timeout=5)\n",
    "    \n",
    "    # If the GET request is successful, scrape the data\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the page\n",
    "        page_content = response.content\n",
    "        \n",
    "        # Create a BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        \n",
    "        # Find all elements on the page that contain text\n",
    "        for element in soup.find_all(text=True):\n",
    "            # Remove leading/trailing whitespace from the text\n",
    "            text = element.strip()\n",
    "            \n",
    "            # If the text is not empty, add it to the scraped data dictionary\n",
    "            if text:\n",
    "                scraped_data[text] = updated_url\n",
    "                \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for TaxID {tax_id}: status code {response.status_code}\")\n",
    "        \n",
    "    # Add a delay to avoid overwhelming the server\n",
    "    sleep(1)\n",
    "\n",
    "# Print the scraped data\n",
    "for key, value in scraped_data.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "     \"\"\"\n",
    "    }\n",
    "]\n",
    "#create payload\n",
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "#print the result\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9193dc-64f5-417f-9cd6-79e2c4580c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated URLs:\n",
      "https://doanhnghiep.biz/3502447502\n",
      "https://doanhnghiep.biz/030352220\n"
     ]
    }
   ],
   "source": [
    "# Define the base URL and list of TaxIDs\n",
    "base_url = \"https://doanhnghiep.biz/\"\n",
    "tax_ids = [\"3502447502\", \"030352220\"]\n",
    "\n",
    "# Create an empty list to store the updated URLs\n",
    "updated_urls = []\n",
    "\n",
    "# Iterate over each TaxID and update the URL\n",
    "for tax_id in tax_ids:\n",
    "    updated_url = base_url + str(tax_id)\n",
    "    updated_urls.append(updated_url)\n",
    "\n",
    "# Print the updated URLs\n",
    "print(\"Updated URLs:\")\n",
    "for url in updated_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53dc1742-12a1-496f-98a9-bc479a915410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for TaxID 3502447502: status code 403\n",
      "Failed to retrieve data for TaxID 030352220: status code 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from time import sleep\n",
    "\n",
    "# Define the base URL and list of TaxIDs\n",
    "base_url = \"https://doanhnghiep.biz/\"\n",
    "tax_ids = [\"3502447502\", \"030352220\"]\n",
    "\n",
    "# Create an empty dictionary to store the scraped data\n",
    "scraped_data = {}\n",
    "\n",
    "# Iterate over each TaxID and update the URL\n",
    "for tax_id in tax_ids:\n",
    "    updated_url = base_url + str(tax_id)\n",
    "    \n",
    "    # Send a GET request to the updated URL\n",
    "    response = requests.get(updated_url, timeout=5)\n",
    "    \n",
    "    # If the GET request is successful, scrape the data\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the page\n",
    "        page_content = response.content\n",
    "        \n",
    "        # Create a BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        \n",
    "        # Find all elements on the page that contain text\n",
    "        for element in soup.find_all(text=True):\n",
    "            # Remove leading/trailing whitespace from the text\n",
    "            text = element.strip()\n",
    "            \n",
    "            # If the text is not empty, add it to the scraped data dictionary\n",
    "            if text:\n",
    "                scraped_data[text] = updated_url\n",
    "                \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for TaxID {tax_id}: status code {response.status_code}\")\n",
    "        \n",
    "    # Add a delay to avoid overwhelming the server\n",
    "    sleep(1)\n",
    "\n",
    "# Print the scraped data\n",
    "for key, value in scraped_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64a438d5-e838-415d-bd8e-a4bbc39889c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m updated_url \u001b[38;5;241m=\u001b[39m base_url \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tax_id)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Send a GET request to the updated URL with headers\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(updated_url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If the GET request is successful, scrape the data\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Get the content of the page\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from time import sleep\n",
    "\n",
    "class Website:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Connection': 'keep-alive',\n",
    "        }\n",
    "\n",
    "# Define the base URL and list of TaxIDs\n",
    "base_url = \"https://doanhnghiep.biz/\"\n",
    "tax_ids = [\"3502447502\", \"030352220\"]\n",
    "\n",
    "# Create an empty dictionary to store the scraped data\n",
    "scraped_data = {}\n",
    "\n",
    "# Iterate over each TaxID and update the URL\n",
    "for tax_id in tax_ids:\n",
    "    updated_url = base_url + str(tax_id)\n",
    "    \n",
    "    # Send a GET request to the updated URL with headers\n",
    "    response = requests.get(updated_url, headers=self.headers, timeout=5)\n",
    "    \n",
    "    # If the GET request is successful, scrape the data\n",
    "    if response.status_code == 200:\n",
    "        # Get the content of the page\n",
    "        page_content = response.content\n",
    "        \n",
    "        # Create a BeautifulSoup object and specify the parser\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "        \n",
    "        # Find all elements on the page that contain text\n",
    "        for element in soup.find_all(text=True):\n",
    "            # Remove leading/trailing whitespace from the text\n",
    "            text = element.strip()\n",
    "            \n",
    "            # If the text is not empty, add it to the scraped data dictionary\n",
    "            if text:\n",
    "                scraped_data[text] = updated_url\n",
    "                \n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for TaxID {tax_id}: status code {response.status_code}\")\n",
    "        \n",
    "    # Add a delay to avoid overwhelming the server\n",
    "    sleep(1)\n",
    "\n",
    "# Print the scraped data\n",
    "for key, value in scraped_data.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745adc6a-8a35-4a9f-9519-2bd584683723",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "     create a python program to adding website + TaxID   \n",
    "I have a list of TaxID as below     \n",
    "List TaxID = 5300829397, 5300773641\n",
    "\n",
    "The website is https://doanhnghiep.biz/\n",
    "\n",
    "Example result is added to url list\n",
    "https://doanhnghiep.biz/5300829397\n",
    "https://doanhnghiep.biz/5300773641\n",
    "     \"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
